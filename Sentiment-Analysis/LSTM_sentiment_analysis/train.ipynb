{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocabs, embedding_size, tag_size, hidden_size, rnn_layers, dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        word2id, tag2id = vocabs\n",
    "        self.word_embeddings = nn.Embedding(len(word2id), embedding_size)\n",
    "        self.tag_embeddings = nn.Embedding(len(tag2id), tag_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_size + tag_size, hidden_size, rnn_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size * 2, 1)\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x_word, x_tag):\n",
    "        x_word_emb = self.word_embeddings(x_word)\n",
    "        x_tag_emb = self.tag_embeddings(x_tag)\n",
    "\n",
    "        rnn_input = torch.cat([x_word_emb, x_tag_emb], dim=-1)\n",
    "\n",
    "        rnn_input = F.dropout(rnn_input, self.dropout_rate, self.training)\n",
    "\n",
    "        outputs, (hn, cn) = self.lstm(rnn_input)\n",
    "\n",
    "        output = torch.cat([hn[0], hn[1]], dim=-1)\n",
    "\n",
    "        output = torch.sigmoid(self.linear(output)).squeeze(1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with open(\"data.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    trainset, testset, word2id, tag2id = data['train'], data['test'], data['w2id'], data['t2id']\n",
    "\n",
    "    model = Model((word2id, tag2id), embedding_size=300, tag_size=50, hidden_size=300, rnn_layers=1, dropout_rate=0.3)\n",
    "    model.to(device)\n",
    "\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Binary CrossEntropy Loss\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    optimizer = optim.Adam(parameters)\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 20\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        num_data = len(trainset)\n",
    "        num_batch = (num_data + batch_size - 1) // batch_size\n",
    "\n",
    "        model.train()\n",
    "        for ii in range(num_batch):\n",
    "            start = ii * batch_size\n",
    "            end = num_data if (ii + 1) * batch_size > num_data else (ii + 1) * batch_size\n",
    "\n",
    "            batch_data = trainset[start:end]\n",
    "\n",
    "            batch_word_ids = [torch.tensor(data[0], dtype=torch.long) for data in batch_data]\n",
    "            batch_tag_ids = [torch.tensor(data[1], dtype=torch.long) for data in batch_data]\n",
    "            batch_labels_ids = [data[2] for data in batch_data]\n",
    "\n",
    "            batch_word_ids = pad_sequence(batch_word_ids, batch_first=True)\n",
    "            batch_tag_ids = pad_sequence(batch_tag_ids, batch_first=True)\n",
    "\n",
    "            batch_labels_ids = torch.tensor(batch_labels_ids, dtype=torch.float)\n",
    "\n",
    "            batch_word_ids = batch_word_ids.to(device)\n",
    "            batch_tag_ids = batch_tag_ids.to(device)\n",
    "            batch_labels_ids = batch_labels_ids.to(device)\n",
    "\n",
    "            batch_outputs = model(batch_word_ids, batch_tag_ids)\n",
    "\n",
    "            loss = criterion(batch_outputs, batch_labels_ids)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.data)\n",
    "\n",
    "            if (ii + 1) % 200 == 0:\n",
    "                print(\"%6d/%6d: loss %.6f\" % (ii + 1, num_batch, sum(losses) / len(losses)))\n",
    "                losses = []\n",
    "\n",
    "        num_data = len(testset)\n",
    "        num_batch = (num_data + batch_size - 1) // batch_size\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total = len(testset)\n",
    "        match = 0\n",
    "        for ii in range(num_batch):\n",
    "            start = ii * batch_size\n",
    "            end = num_data if (ii + 1) * batch_size > num_data else (ii + 1) * batch_size\n",
    "\n",
    "            batch_data = testset[start:end]\n",
    "\n",
    "            batch_word_ids = [torch.tensor(data[0], dtype=torch.long) for data in batch_data]\n",
    "            batch_tag_ids = [torch.tensor(data[1], dtype=torch.long) for data in batch_data]\n",
    "            batch_labels_ids = [data[2] for data in batch_data]\n",
    "\n",
    "            batch_word_ids = pad_sequence(batch_word_ids, batch_first=True)\n",
    "            batch_tag_ids = pad_sequence(batch_tag_ids, batch_first=True)\n",
    "\n",
    "            batch_word_ids = batch_word_ids.to(device)\n",
    "            batch_tag_ids = batch_tag_ids.to(device)\n",
    "\n",
    "            batch_outputs = model(batch_word_ids, batch_tag_ids)\n",
    "\n",
    "            batch_outputs = batch_outputs.data.cpu().numpy().tolist()\n",
    "\n",
    "            batch_pred_ids = [1 if output >= 0.5 else 0 for output in batch_outputs]\n",
    "\n",
    "            for a, o in zip(batch_labels_ids, batch_pred_ids):\n",
    "                if a == o:\n",
    "                    match += 1\n",
    "\n",
    "        print(\"Epoch %d, match : %6d, total : %6d, ACC : %.2f\" % (epoch, match, total, 100 * match / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-1611d69a6aff>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'w2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.pkl'"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
