{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofbzn9tt9A9"
      },
      "source": [
        "import os\n",
        "import operator\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koatr6qZt_Cr"
      },
      "source": [
        "def read_data(filename):\n",
        "    dataset = []\n",
        "    sent = []\n",
        "    for line in open(filename):\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line:\n",
        "            doc_id, label = sent[-1].split(\"\\t\")\n",
        "\n",
        "            data = ([item.split(\" \") for item in sent[:-1]], int(label))\n",
        "            dataset.append(data)\n",
        "            sent = []\n",
        "            continue\n",
        "\n",
        "        sent.append(line)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xes9sBDTDhzP"
      },
      "source": [
        "def make_vocab(dataset):\n",
        "    words = {}\n",
        "    tags = {}\n",
        "\n",
        "    for data in dataset:\n",
        "        sent, label = data[0], data[1]\n",
        "\n",
        "        for m, t in sent:\n",
        "            if m not in words:\n",
        "                words[m] = 0\n",
        "            words[m] += 1\n",
        "\n",
        "            if t not in tags:\n",
        "                tags[t] = 0\n",
        "            tags[t] += 1\n",
        "\n",
        "    sorted_words = sorted(words.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    sorted_tags = sorted(tags.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "    word2id = {w: i + 2 for i, (w, c) in enumerate(sorted_words)}\n",
        "    tag2id = {w: i for i, (w, c) in enumerate(sorted_tags)}\n",
        "\n",
        "    word2id['<PAD>'] = 0\n",
        "    word2id['<UNK>'] = 1\n",
        "\n",
        "    return word2id, tag2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cywNZ5fWDiaW"
      },
      "source": [
        "def convert_ids(word2id, tag2id, data, UNK=1):\n",
        "    sent, label = data[0], data[1]\n",
        "\n",
        "    word_ids = [word2id[w] if w in word2id else UNK for w, t in sent]\n",
        "    tag_ids = [tag2id[t] if t in tag2id else UNK for w, t in sent]\n",
        "\n",
        "    return word_ids, tag_ids, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZsmxSoGDjoF"
      },
      "source": [
        "def main():\n",
        "    traindata = read_data(os.path.join(\"data\", \"train_tagged.txt\"))\n",
        "    testdata = read_data(os.path.join(\"data\", \"test_tagged.txt\"))\n",
        "\n",
        "    word2id, tag2id = make_vocab(traindata)\n",
        "\n",
        "    train = [convert_ids(word2id, tag2id, data) for data in traindata]\n",
        "    test = [convert_ids(word2id, tag2id, data) for data in testdata]\n",
        "\n",
        "    data = {'train': train, 'test': test, 'w2id': word2id, 't2id': tag2id}\n",
        "\n",
        "    with open('data.pkl', 'wb') as f:\n",
        "        pickle.dump(data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJDS53fFDlm9"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}